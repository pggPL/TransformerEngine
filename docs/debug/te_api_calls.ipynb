{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calls to Nvidia-DLFramework-Inspect\n",
    "\n",
    "Let's refresh how Nvidia-DLFramework-Inspect with Transformer Engine work together. TransformerEngine layers initialized with `debug=True` have some hook calls inside each of the GEMMs. User can define feature classes or use feature classes provided with TE. File `config.yaml` describes which hooks need to be used to which layers. Nvidia-DLFramework-Inspect combines 3 things: TE training, feature classes and `config.yaml` and takes care of inserting hooks in correct places. This process can be seen in the image below.\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"./img/api_calls.svg\">\n",
    "    <figcaption> Fig 1: Example of Nvidia-DLFramework-Inspect affecting traing script with 3 TE Linear Layers. There are 2 feature classes defined. There is specification in <i>config.yaml</i>for each layers which features should be used. In \"Layer 1\" function \"process tensor\" from the Feature 2 is inserted and in \"Layer 2\" there is process_tensor() from Feature 1. \"Layer 3\" is not affected. </figcaption>\n",
    "</figure>\n",
    "\n",
    "In this page all calls from TransformerEngine to the Nvidia-DLFramework-Inspect for each GEMM are listed. Order of these calls can be seen on image below.\n",
    "\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"./img/api_calls2.svg\">\n",
    "    <figcaption> Fig 2: The calls to Nvidia-DLFramework-Inspect done for one GEMM in the Transformer Engine when layer has `debug=True`. `fp8_gemm()` call is used to determine if GEMM will be done in FP8 or in the high precision. If it returns <i>False</i>, then calls related to quantization are not invoked.</figcaption>\n",
    "</figure>\n",
    "\n",
    "### process_tensor\n",
    "\n",
    "This call is invoked before and after every GEMM. It allows to insert some tensor processing. For example feature `FakeCastFp8` use it to emulate casting to FP8, but the tensor is returned in higher precision.\n",
    "\n",
    "Args:\n",
    "\n",
    "- `tensor: torch.tensor`,\n",
    "- `gemm: str` – one of [`fprop`, `dgrad`, `wgrad`],\n",
    "- `tensor_name: str` – one of [`activation`, `weight`, `gradient`, `output`, `wgrad`, `dgrad`].\n",
    "\n",
    "Should return:\n",
    "\n",
    "- `processed_tensor: torch.tensor` – tensor after processing.\n",
    "\n",
    "### process_quantized_tensor\n",
    "\n",
    "This call allows to process the tensor after the quantization.\n",
    "\n",
    "Args:\n",
    "\n",
    "- `tensor: transformer_engine.pytorch.QuantizedTensor`,\n",
    "- `gemm: str` – one of [`fprop`, `dgrad`, `wgrad`],\n",
    "- `tensor_name: str` – one of [`activation`, `weight`, `gradient`, `output`, `wgrad`, `dgrad`].\n",
    "\n",
    "Should return:\n",
    "\n",
    "- `processed_tensor: torch.tensor` – tensor after processing.\n",
    "\n",
    "### fp8_gemm\n",
    "\n",
    "It is used to determine whether GEMM will be proceeded in FP8 or in high precision. It is called only if forward is inside enabled FP8 autocast.\n",
    "\n",
    "Args:\n",
    "\n",
    "- `gemm: str` – one of [`fprop`, `dgrad`, `wgrad`],\n",
    "\n",
    "Should return:\n",
    "\n",
    "- `fp_gemm: bool` – tensor after processing.\n",
    "\n",
    "### save_stats_for_logging_quantized\n",
    "\n",
    "This call allows to get the tensor after the quantization, it is used by the feature `LogFp8TensorStats`.\n",
    "\n",
    "Args:\n",
    "\n",
    "- `tensor: transformer_engine.pytorch.QuantizedTensor`,\n",
    "- `gemm: str` – one of [`fprop`, `dgrad`, `wgrad`],\n",
    "- `tensor_name: str` – one of [`activation`, `weight`, `gradient`, `output`, `wgrad`, `dgrad`].\n",
    "\n",
    "Should return nothing.\n",
    "\n",
    "### save_stats_for_logging\n",
    "\n",
    "\n",
    "This call allows to get the stats from the tensor, it is used by the feature `LogTensorStats`.\n",
    "\n",
    "Args:\n",
    "\n",
    "- `tensor: torch.tensor`,\n",
    "- `gemm: str` – one of [`fprop`, `dgrad`, `wgrad`],\n",
    "- `tensor_name: str` – one of [`activation`, `weight`, `gradient`, `output`, `wgrad`, `dgrad`].\n",
    "\n",
    "Should return nothing.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
