{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing new precision with the Megatron-LM\n",
    "\n",
    "In this tutorial we present how to test new precision/recipe using Megatron-LM and Nvidia-DLFramework-Inspect. \n",
    "\n",
    "[Megatron-LM](https://github.com/NVIDIA/Megatron-LM) is a large-scale transformer model framework developed by NVIDIA for training natural language processing (NLP) models with billions of parameters. It is designed to optimize both model training efficiency and scalability, allowing researchers and developers to push the limits of NLP capabilities.\n",
    "\n",
    "We will show how to test ideas for new precisions/recipes in few simple steps.\n",
    "\n",
    "\n",
    "Consider some example precisions/recipe for training GPT-like Transformer:\n",
    "\n",
    "1. Each weight to the GEMM is casted to -1/0/1 with scaling factor. For the sake of this tutorial, suppose we have implemented function `utils.zero_one_cast(x: torch.Tensor) -> (torch.Tensor, float)` which returns `-1/0/1` tensor and scaling factor as a float. \n",
    "2. Each input is casted to 4E3M FP8 precision.\n",
    "\n",
    "We will present how to <b>emulate</b> behaviour of such recipe - tensors will be casted to this precisions and then casted back to high precision. Then GEMM will be done also in high precision.\n",
    "\n",
    "Moreover let's suppose we want to test few scenarios:\n",
    "1. Use new precision for both backward and forward.\n",
    "2. Use new precision only for forward and use FP8 for backward.\n",
    "3. Use new precision for both forward and backward, but one in every 5 consecutive layers will be run in high precision.\n",
    "\n",
    "We will present how to implement it using the Transformer Engine.\n",
    "\n",
    "#### Feature class implementation\n",
    "\n",
    "Let's look how feature `FakeFP8Cast` looks like.\n",
    "\n",
    "```python\n",
    "\n",
    "@Registry.register_feature(namespace=\"transformer_engine\")\n",
    "@append_parent_docstring(parent=TEConfigAPIMapper)\n",
    "class FakeQuantFp8(TEConfigAPIMapper):\n",
    "    # (...)\n",
    "    \n",
    "    @api_method\n",
    "    def fp8_gemm(self, config, layer_name, gemm, **kwargs):\n",
    "        return False\n",
    "\n",
    "    @api_method\n",
    "    def process_tensor(self, config, layer_name, gemm, **kwargs):\n",
    "        # (...)\n",
    "        \n",
    "        quant_format = config[\"quant_format\"]\n",
    "        margin = config.get('margin', self._get_margin_default())\n",
    "        q_tensor = fake_quantize_fp8(kwargs[\"tensor\"], quant_format, margin=margin)\n",
    "        return q_tensor\n",
    "\n",
    "```\n",
    "\n",
    "We will make something similar - but using `utils.zero_one_cast` in the case of the weight:\n",
    "```python\n",
    "\n",
    "@Registry.register_feature(namespace=\"transformer_engine\")\n",
    "@append_parent_docstring(parent=TEConfigAPIMapper)\n",
    "class NewRecipe(TEConfigAPIMapper):\n",
    "    # (...)\n",
    "    \n",
    "    @api_method\n",
    "    def fp8_gemm(self, config, layer_name, gemm, tensor_name, **kwargs):\n",
    "        return False\n",
    "\n",
    "    @api_method\n",
    "    def process_tensor(self, config, layer_name, tensor_name, gemm, **kwargs):\n",
    "        # (...)\n",
    "        if tensor_name == \"weight:\n",
    "            return utils.zero_one_cast(kwargs[\"tensor\"])\n",
    "        else:\n",
    "            quant_format = config[\"quant_format\"]\n",
    "            margin = config.get('margin', self._get_margin_default())\n",
    "            q_tensor = fake_quantize_fp8(kwargs[\"tensor\"], quant_format, margin=margin)\n",
    "            return q_tensor\n",
    "```\n",
    "\n",
    "Suppose that our feature is saved in the dir `/path/to/feature/new_precision.py`.\n",
    "\n",
    "#### Integration with the Megatron-LM\n",
    "\n",
    "We have succesfully defined our feature, which disabled FP8 GEMM and runs high precision GEMM with fake-casted \n",
    "tensors, emulating new precision.\n",
    "\n",
    "Now, let's look how to use our recipe with Megatron-LM training.\n",
    "Let's begin with preparing some `config.yaml` file to make experiments in different scenarios.\n",
    "\n",
    "```yaml\n",
    "Experiment1:\n",
    "  enabled: True # Experiment 1 is now enabled, one needs to change it manually to enable other experiment\n",
    "  layers:\n",
    "    layer_name_regex_pattern: '*'\n",
    "  transformer_engine:\n",
    "    new_recipe:\n",
    "      enabled: True\n",
    "      gemms: [fprop, dgrad, wgrad] # forward and backward\n",
    "Experiment2:\n",
    "  enabled: False\n",
    "  layers:\n",
    "    layer_name_regex_pattern: '*'\n",
    "  transformer_engine:\n",
    "    new_recipe:\n",
    "      enabled: True\n",
    "      gemms: [fprop] # forward\n",
    "Experiment3:\n",
    "  enabled: False\n",
    "  layers:\n",
    "    layer_name_regex_pattern: '*[1|2|3|4|6|7|8|9]' # four of every 5 layers\n",
    "  transformer_engine:\n",
    "    new_recipe:\n",
    "      enabled: True\n",
    "      gemms: [fprop, dgrad, wgrad] # forward and backward\n",
    "\n",
    "```\n",
    "\n",
    "1. Provide path to directory containing our feature using `NV_DEBUG_TOOL_FEATURE_DIRS` environment variable.\n",
    "```bash\n",
    "export NV_DEBUG_TOOL_FEATURE_DIRS=\"/path/to/feature\"\n",
    "```\n",
    "\n",
    "2. Provide the path to a config using the `NV_DEBUG_TOOL_CONFIG` environment variable. If config is not provided, it will run with default setting.\n",
    "```bash\n",
    "export NV_DEBUG_TOOL_CONFIG=\"/path/to/config.yaml\"\n",
    "```\n",
    "\n",
    "3. Use the `NV_DEBUG_TOOL_ENABLED` environment variable to toggle debug mode in Megatron-LM.\n",
    "```sh\n",
    "export NV_DEBUG_TOOL_ENABLED=1\n",
    "```\n",
    "4. Run Megatron-LM script with model you want to train. That's it!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
