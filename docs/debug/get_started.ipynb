{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Warning</b>\n",
    "\n",
    "Precision debug tools with Nvidia-DLFramework-Inspect for Transformer Engine is currently supported only for Torch.\n",
    "\n",
    "</div>\n",
    "\n",
    "Transformer Engine supports FP8 training, which can significantly speed up training for most transformers without any noticeable difference in accuracy compared to higher precision. However, in some cases, reducing precision may lead to a drop in accuracy or cause instability during training. To address this, we provide specialized precision debug tools that help identify potential issues. They allow, for example:\n",
    "\n",
    "- switching a specific GEMM operation from FP8 to higher precision,\n",
    "- selecting between immediate or delayed scaling of FP8 tensors,\n",
    "- detailed logging of tensor statistics.\n",
    "\n",
    "Moreover one can use Nvidia-DLFramework-Inspect with Transformer Engine to add own debug features. Fake casting to user-defined precision is described in (link)[link].\n",
    "\n",
    "There are 3 things one needs to do to use Transformer Engine debug features:\n",
    "\n",
    "1. Define `TransformerLayer` and other TE layers with option `debug=True`.\n",
    "2. Create a **config.yaml** file to configure the desired features.\n",
    "3. Install, import and initialize Nvidia-DLFramework-Inspect tool.\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"./img/introduction.svg\">\n",
    "    <figcaption> Fig 1: To use debug features, one needs to init the Transformer Layer with `debug=True` and specify the configuration in the `config.yaml` file. This configuration can affect the behavior of the GEMMs. </figcaption>\n",
    "</figure>\n",
    "\n",
    "Let's look at a simple example of training a Transformer layer using Transformer Engine with FP8 precision. This example demonstrates how to set up the layer, define an optimizer, and perform a few training iterations using dummy data.\n",
    "\n",
    "```python\n",
    "# train.py\n",
    "\n",
    "from transformer_engine.pytorch import TransformerLayer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformer_engine as te\n",
    "\n",
    "hidden_size = 512\n",
    "num_attention_heads = 8\n",
    "\n",
    "transformer_layer = TransformerLayer(\n",
    "    hidden_size=hidden_size,\n",
    "    num_attention_heads=num_attention_heads\n",
    ").cuda()\n",
    "\n",
    "dummy_input = torch.randn(10, 32, hidden_size).cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(transformer_layer.parameters(), lr=1e-4)\n",
    "dummy_target = torch.randn(10, 32, hidden_size).cuda()\n",
    "\n",
    "for epoch in range(5):\n",
    "    transformer_layer.train()\n",
    "    optimizer.zero_grad()\n",
    "    with te.fp8_autocast(enabled=True):\n",
    "        output = transformer_layer(dummy_input)\n",
    "    loss = criterion(output, dummy_target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "We will demonstrate two debug features on the code above:\n",
    "\n",
    "1. Disabling FP8 precision for a specific GEMM operations, such as the FC1 and FC2 forward propagation GEMM.\n",
    "2. Logging statistics for other GEMM operations, such as gradient statistics for dgradÂ GEMM within the LayerNormLinear layer.\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "To use the debug features of Transformer Engine, you need to install the (Nvidia-DLFramework-Inspect)[link] package provided by NVIDIA. You can install it by following these steps:\n",
    "\n",
    "```\n",
    "git clone [link]\n",
    "cd Nvidia-DLFramework-Inspect\n",
    "pip install .\n",
    "```\n",
    "\n",
    "#### Config file\n",
    "\n",
    "We need to prepare **config.yaml** file, as below\n",
    "\n",
    "```yaml\n",
    "# config.yaml\n",
    "\n",
    "fc1_fprop_to_fp8:\n",
    "  enabled: True\n",
    "  layers:\n",
    "    layer_types: [fc1, fc2] # contains fc1 or fc2 in name\n",
    "  transformer_engine:\n",
    "    DisableFp8Gemm:\n",
    "      enabled: True\n",
    "      gemms: [fprop]\n",
    "\n",
    "log_tensor_stats:\n",
    "  enabled: True\n",
    "  layers:\n",
    "    layer_types: [layernorm_linear] # contains layernorm_linear in name\n",
    "  transformer_engine:\n",
    "    LogTensorStats:\n",
    "      enabled: True\n",
    "      stats: [max, min, mean, std, l1_norm]\n",
    "      tensors: [activation]\n",
    "      freq: 1\n",
    "      start_step: 2\n",
    "      end_step: 5\n",
    "```\n",
    "\n",
    "Further explanation on how to create config files is in the next section of this tutorial.\n",
    "\n",
    "#### Adjusting Python file\n",
    "\n",
    "```python\n",
    "# (...)\n",
    "\n",
    "import nvtorch_inspect.api as nvinspect_api\n",
    "nvinspect_api.initialize(\n",
    "    config_file=\"./config.yaml\",\n",
    "    feature_dirs=[\"/path/to/transformer_engine/debug/features\"],\n",
    "    log_dir=\"./log\",\n",
    "    default_logging_enabled=True)\n",
    "\n",
    "# (...)\n",
    "\n",
    "transformer_layer = TransformerLayer(..., debug=True, debug_name=\"transformer_layer\").cuda()\n",
    "\n",
    "# (...)\n",
    "```\n",
    "\n",
    "In the modified code above, the following changes were made:\n",
    "\n",
    "1. Added an import for `nvtorch_inspect.api`.\n",
    "2. Replaced the original `TransformerLayer` import with the debug version from `transformer_engine.debug.pytorch`.\n",
    "3. Initialized the Nvidia-DLFramework-Inspect by calling `nvinspect_api.initialize()` with appropriate configuration, specifying the path to the config file, feature directories, and log directory.\n",
    "4. Modified the instantiation of `TransformerLayer` to include the `name` parameter (`name=\"transformer_layer\"`). This helps in identifying the specific layer for debugging purposes.\n",
    "\n",
    "#### Inspecting the logs\n",
    "\n",
    "Let's look at the files with the logs. Two files will be created:\n",
    "\n",
    "1. First for main debug logs.\n",
    "2. Second for statistics logs.\n",
    "\n",
    "Let's look inside them!\n",
    "\n",
    "```\n",
    "# log/debug_logs/debug_log_globalrank-0.log\n",
    "\n",
    "INFO - [DEBUG-INFO] Default logging to file enabled at ./log\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv: Fprop Activation: Delayed Scaling\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv: Fprop Weight: Delayed Scaling\n",
    "INFO - transformer_layer.self_attn.proj: Fprop Activation: Delayed Scaling\n",
    "INFO - transformer_layer.self_attn.proj: Fprop Weight: Delayed Scaling\n",
    "INFO - transformer_layer.layernorm_mlp.fc1: Feature=DisableFp8Gemm, API=is_fp8_gemm_enabled: fprop: FP8 GEMM: False\n",
    "INFO - transformer_layer.layernorm_mlp.fc1: Fprop: BF16\n",
    "INFO - transformer_layer.layernorm_mlp.fc2: Feature=DisableFp8Gemm, API=is_fp8_gemm_enabled: fprop: FP8 GEMM: False\n",
    "INFO - transformer_layer.layernorm_mlp.fc2: Fprop: BF16\n",
    "INFO - transformer_layer.layernorm_mlp.fc2: Fprop Activation: Delayed Scaling\n",
    "INFO - transformer_layer.layernorm_mlp.fc2: Fprop Weight: Delayed Scaling\n",
    "INFO - transformer_layer.layernorm_mlp.fc2: Dgrad Gradient: Delayed Scaling\n",
    "INFO - transformer_layer.layernorm_mlp.fc2: Dgrad Weight: Delayed Scaling\n",
    "INFO - transformer_layer.layernorm_mlp.fc2: Wgrad Gradient: Delayed Scaling\n",
    "INFO - transformer_layer.layernorm_mlp.fc2: Wgrad Activation: Delayed Scaling\n",
    "INFO - transformer_layer.layernorm_mlp.fc1: Dgrad Gradient: Delayed Scaling\n",
    "INFO - transformer_layer.layernorm_mlp.fc1: Dgrad Weight: Delayed Scaling\n",
    "INFO - transformer_layer.layernorm_mlp.fc1: Wgrad Gradient: Delayed Scaling\n",
    "INFO - transformer_layer.layernorm_mlp.fc1: Wgrad Activation: Delayed Scaling\n",
    "INFO - transformer_layer.self_attn.proj: Dgrad Gradient: Delayed Scaling\n",
    "INFO - transformer_layer.self_attn.proj: Dgrad Weight: Delayed Scaling\n",
    "INFO - transformer_layer.self_attn.proj: Wgrad Gradient: Delayed Scaling\n",
    "INFO - transformer_layer.self_attn.proj: Wgrad Activation: Delayed Scaling\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv: Dgrad Gradient: Delayed Scaling\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv: Dgrad Weight: Delayed Scaling\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv: Wgrad Gradient: Delayed Scaling\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv: Wgrad Activation: Delayed Scaling\n",
    "....\n",
    "```\n",
    "\n",
    "In the main log file, you can find detailed information about the transformer's layer GEMMs behavior. You can see that `fc1` and `fc2` fprop GEMMs are run in high precision, as intended.\n",
    "\n",
    "```\n",
    "# log/debug_statistics_logs/statistics_log_globalrank-0.log\n",
    "\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv_activation_max              iteration=000002 \t\t\t\t value=4.4874\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv_activation_min              iteration=000002 \t\t\t\t value=-4.1867\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv_activation_mean             iteration=000002 \t\t\t\t value=-0.0000\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv_activation_std              iteration=000002 \t\t\t\t value=0.9999\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv_activation_l1_norm          iteration=000002 \t\t\t\t value=130665.7031\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv_activation_max              iteration=000003 \t\t\t\t value=4.4872\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv_activation_min              iteration=000003 \t\t\t\t value=-4.1864\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv_activation_mean             iteration=000003 \t\t\t\t value=-0.0000\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv_activation_std              iteration=000003 \t\t\t\t value=0.9998\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv_activation_l1_norm          iteration=000003 \t\t\t\t value=130654.3047\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv_activation_max              iteration=000004 \t\t\t\t value=4.4872\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv_activation_min              iteration=000004 \t\t\t\t value=-4.1861\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv_activation_mean             iteration=000004 \t\t\t\t value=-0.0000\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv_activation_std              iteration=000004 \t\t\t\t value=0.9997\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv_activation_l1_norm          iteration=000004 \t\t\t\t value=130643.2422\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv_activation_max              iteration=000005 \t\t\t\t value=4.4876\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv_activation_min              iteration=000005 \t\t\t\t value=-4.1858\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv_activation_mean             iteration=000005 \t\t\t\t value=-0.0000\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv_activation_std              iteration=000005 \t\t\t\t value=0.9997\n",
    "INFO - transformer_layer.self_attn.layernorm_linear_qkv_activation_l1_norm          iteration=000005 \t\t\t\t value=130632.5781\n",
    "```\n",
    "\n",
    "The second log file (`statistics_log_globalrank-0.log`) contains statistics for tensors we requested in `config.yaml`.\n",
    "\n",
    "\n",
    "#### Logging using TensorBoard\n",
    "\n",
    "Precision debug tools supports logging using [TensorBoard](https://www.tensorflow.org/tensorboard). To enable it, one needs to pass the argument `tb_writer` to the `nvinspect_api.initialize()`.  Let's modify `train.py` file.\n",
    "\n",
    "```python\n",
    "\n",
    "# (...)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tb_writer = SummaryWriter('./tensorboard_dir/run1')\n",
    "\n",
    "# add tb_writer to the Debug API initialization\n",
    "nvinspect_api.initialize(\n",
    "    config_file=\"./config.yaml\",\n",
    "    feature_dirs=[\"/path/to/transformer_engine/debug/features\"],\n",
    "    log_dir=\"./log\",\n",
    "    tb_writer=tb_writer)\n",
    "\n",
    "# (...)\n",
    "```\n",
    "\n",
    "Let's run training and open TensorBoard by `tensorboard --logdir=./tensorboard_dir/run1`:\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"./img/tensorboard.png\">\n",
    "    <figcaption> Fig 2: TensorBoard with plotted stats.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "Transformer Engine's precision debug tools help address FP8 training issues. Properly setting up the configuration files and debug layers will help you gain better insights into model behavior and optimize the training process.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
