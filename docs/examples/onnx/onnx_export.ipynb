{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to ONNX and inference using TensorRT\n",
    "\n",
    "Transformer Engine is the library supporting the low precision training. But the training is one part of the story, another is the inference. There are multiple ways of transfering models trained in TE to inference software. If the model is trained used NeMo, one can export it directly to the TRT-LMM format - refer for this [tutorial](link). We provide another way of transfering model out of the Transformer Engine - namely export to the ONNX format with custom FP8 extensions. \n",
    "\n",
    "When to use export to the ONNX?\n",
    "\n",
    "- if Transformer Engine is only part of the bigger model - PyTorch ONNX export API allows us to easily export such models,\n",
    "- when KV cache is not needed - encoder only use cases - like in this tutorial.\n",
    "\n",
    "Perfect example when exporting model to the ONNX and using TensorRT for inference are the diffusion models.\n",
    "Below we provide example implementation of one of such models.\n",
    "\n",
    "```python3\n",
    "\n",
    "class DiffusionModel(torch.nn.Module):\n",
    "   ....\n",
    "\n",
    "```\n",
    "\n",
    "Let's see how fast the inference in the raw TE is.\n",
    "\n",
    "```python3\n",
    "# time measue\n",
    "for _ in range(...):\n",
    "    model(img_enc, text_enc)\n",
    "\n",
    "```\n",
    "\n",
    "#### Exporting the TE model to the ONNX format\n",
    "\n",
    "To export TE model to the ONNX format one needs to \n",
    "\n",
    "- use warm-up  runs in correct autocasts,\n",
    "- wrap exporting code in `te.onnx_export`,\n",
    "- use Pytorch dynamo onnx exporter `torch.onnx.export(..., dynamo=True)`\n",
    "\n",
    "```python3\n",
    "\n",
    "with te.onnx_export():\n",
    "    onnx_model = torch.onnx.export(..., dynamo=True)\n",
    "\n",
    "```\n",
    "\n",
    "#### Inference with TensorRT\n",
    "\n",
    "If the model is exported to the ONNX format, it can be loaded by TensorRT. TensorRT is NVIDIA inference software. Let's see at the example of export and engine generation:\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "Now engine needs to be run, let's see how fast it is\n",
    "```python3\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "So it is **1.1x** faster than raw Pytorch/Transformer Engine forward.\n",
    "\n",
    "#### Low precision in ONNX and TensorRT\n",
    "\n",
    "ONNX standard does not support all precisions supposted by the Transformer Engine. One can see all onnx operators in [this website](https://onnx.ai/onnx/operators/). Thus TensorRT and Transformer Engine use some low precision operators, documented below.\n",
    "\n",
    "**TRT_FP8_QUANTIZE**\n",
    "\n",
    "aaa\n",
    "\n",
    "**TRT_FP8_DEQUANTIZE**\n",
    "\n",
    "aaa\n",
    "\n",
    "\n",
    "Since standard operators do not support input and output in some precisions, we use workaround - we dequantize all tensors before the input to such operators or quantize tensors after such operators. TensorRT detect such patterns and subsitutes them with proper operations.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
